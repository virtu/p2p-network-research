{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read annotated nodes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "ANNOTATED_NODES_CSV = \"reachable-nodes-annotated-2023-05-07.csv\"\n",
    "\n",
    "assert os.path.exists(ANNOTATED_NODES_CSV), \"Annotated node data missing! Run 01_prep_input_data.ipynb to create.\"\n",
    "\n",
    "df = pd.read_csv(ANNOTATED_NODES_CSV)\n",
    "\n",
    "# address statistics\n",
    "num_ipv4 = sum(df.network == 'ipv4')\n",
    "num_ipv6 = sum(df.network == 'ipv6')\n",
    "num_ip = len(df)\n",
    "assert num_ipv4 + num_ipv6 == num_ip, \"Inconsistent dichotomy\"\n",
    "print(f'reachable nodes: ipv4={num_ipv4}, ipv6={num_ipv6}, ipv4/ipv6={num_ip}')\n",
    "\n",
    "# netgroup statistics\n",
    "ip_netgroups = df.netgroup.value_counts().values\n",
    "ipv4_netgroups = df[df.network == 'ipv4'].netgroup.value_counts().values\n",
    "ipv6_netgroups = df[df.network == 'ipv6'].netgroup.value_counts().values\n",
    "assert sum(ipv4_netgroups) + sum(ipv6_netgroups) + sum(ip_netgroups) == 2 * len(df), \"Inconsistent dichotomy!\"\n",
    "print(f'netgroups: ipv4={len(ipv4_netgroups)}, ipv6={len(ipv6_netgroups)}, ipv4/ipv6={len(ip_netgroups)}, total_addresses={sum(ip_netgroups)}')\n",
    "\n",
    "# asn statistics\n",
    "ip_asns = df.asn.value_counts().values\n",
    "ipv4_asns = df[df.network == 'ipv4'].asn.value_counts().values\n",
    "ipv6_asns = df[df.network == 'ipv6'].asn.value_counts().values\n",
    "assert sum(ipv4_asns) + sum(ipv6_asns) + sum(ip_asns) == 2 * len(df), \"Inconsistent dichotomy!\"\n",
    "print(f'asn: ipv4={len(ipv4_asns)}, ipv6={len(ipv6_asns)}, ipv4/ipv6={len(ip_asns)}, total_addresses={sum(ip_asns)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare attack costs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy.stats import hypergeom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "tqdm.pandas()\n",
    "\n",
    "# BiasedUrn R package\n",
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.vectors import IntVector, FloatVector\n",
    "import rpy2.robjects as robjects\n",
    "biasedurn = rpackages.importr('BiasedUrn')\n",
    "\n",
    "def p_eclipse_wallenius(good_bucket_sizes: tuple[int], bad_bucket_size: float, num_bad_buckets: int, num_outbound_conns: int = 10) -> float:\n",
    "    x = IntVector([0] * len(good_bucket_sizes) + [num_outbound_conns])  # number of balls of each color sampled: zero benign balls, number of outgoing conns. bad balls\n",
    "    m = IntVector([1] * len(good_bucket_sizes) + [num_bad_buckets])     # initial number of balls of each color: one ball for each benign color, number of buckets balls for bad color\n",
    "    n = num_outbound_conns                                              # number of balls sampled: number of outbound connections\n",
    "    odds = FloatVector(good_bucket_sizes + tuple([bad_bucket_size]))    # weight for each color, arbitrarily scaled: number of nodes in bucket\n",
    "    p = biasedurn.dMWNCHypergeo(x=x, m=m, n=n, odds=odds)               # returns an R FloatVector that contains one element\n",
    "    return p[0]\n",
    "\n",
    "def p_eclipse_wallenius_fast(good_bucket_sizes: tuple[int], bad_bucket_size: float, num_bad_buckets: int, num_outbound_conns: int = 10) -> float:\n",
    "    x = IntVector([0] + [num_outbound_conns])  # number of balls of each color sampled: zero benign balls, number of outgoing conns. bad balls\n",
    "    m = IntVector([1] + [num_bad_buckets])     # initial number of balls of each color: one ball for each benign color, number of buckets balls for bad color\n",
    "    n = num_outbound_conns                                              # number of balls sampled: number of outbound connections\n",
    "    odds = FloatVector(tuple([sum(good_bucket_sizes)]) + tuple([bad_bucket_size]))    # weight for each color, arbitrarily scaled: number of nodes in bucket\n",
    "    p = biasedurn.dMWNCHypergeo(x=x, m=m, n=n, odds=odds)               # returns an R FloatVector that contains one element\n",
    "    return p[0]\n",
    "\n",
    "\n",
    "def p_eclipse(num_good_nodes: int, num_bad_nodes: int, num_outbound_conns: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Compute eclipse probability (i.e. all outbound to attacker nodes).\n",
    "    Probability distribution is hypergeometric distribution.\n",
    "    \"\"\"\n",
    "    k = num_outbound_conns              # number of observed successes\n",
    "    M = num_good_nodes + num_bad_nodes  # population size (total number of balls)\n",
    "    n = num_bad_nodes                   # number of success states in the population\n",
    "    N = num_outbound_conns              # number of draws\n",
    "    p = hypergeom.pmf(k, M, n, N)\n",
    "    return p\n",
    "\n",
    "\n",
    "def extract(df, ps):\n",
    "    \"\"\"Extract parameters satisfying minimum required probability\"\"\"\n",
    "    data = []\n",
    "    for p in ps:\n",
    "        for num_bad in df.num_bad_nodes.unique().tolist():\n",
    "            df_sel = df[(df.num_bad_nodes == num_bad) & (df.p_netgroup >= p)]\n",
    "            num_bad_netgroups = df_sel.iloc[0].num_bad_buckets if len(df_sel) else np.nan\n",
    "\n",
    "            df_sel = df[(df.num_bad_nodes == num_bad) & (df.p_asn >= p)]\n",
    "            num_bad_asn = df_sel.iloc[0].num_bad_buckets if len(df_sel) else np.nan\n",
    "\n",
    "            data.append({'num_bad_nodes': num_bad, 'p': p, 'num_bad_netgroups': num_bad_netgroups, 'num_bad_asn': num_bad_asn})\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost of IPv4/IPv6 attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "num_bad_nodes = range(1000, 3000000+1, 5000)\n",
    "num_bad_buckets = range(10, 1000, 5)\n",
    "\n",
    "df_in = pd.DataFrame(list(product(num_bad_nodes, num_bad_buckets)), columns=['num_bad_nodes', 'num_bad_buckets'])\n",
    "df_in['bad_bucket_size'] = df_in.num_bad_nodes / df_in.num_bad_buckets\n",
    "df_in['good_netgroup_sizes'] = [tuple(ip_netgroups)] * len(df_in)\n",
    "df_in['good_asn_sizes'] = [tuple(ip_asns)] * len(df_in)\n",
    "\n",
    "# run parameter studies\n",
    "df_in['p_netgroup'] = df_in.progress_apply(lambda x: p_eclipse_wallenius_fast(tuple(x.good_netgroup_sizes), x.bad_bucket_size, x.num_bad_buckets), axis=1)\n",
    "df_in['p_asn'] = df_in.progress_apply(lambda x: p_eclipse_wallenius_fast(tuple(x.good_asn_sizes), x.bad_bucket_size, x.num_bad_buckets), axis=1)\n",
    "\n",
    "# extract bucket sizes and plot\n",
    "df_ip = extract(df_in, ps=[0.1, 0.3, 0.5, 0.75, 0.9])\n",
    "g = sns.lineplot(data=df_ip, x='num_bad_nodes', y='num_bad_asn', hue='p', palette='tab10', alpha=1.00, legend=True)\n",
    "g.set_xscale('log')\n",
    "g.set_yscale('log')\n",
    "g.set_ylabel('Attacker buckets')\n",
    "g.set_xlabel('Attacker nodes')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASMAP-induced cost trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends = []\n",
    "\n",
    "for p in df_ip.p.unique():\n",
    "    d = df_ip[df_ip.p == p].dropna().head(1).to_dict(orient='records')\n",
    "    if d:\n",
    "        locals().update(d[0])\n",
    "        assert num_bad_netgroups == num_bad_asn, \"Internal error!\"\n",
    "        num_bad_nodes_no_bucket = num_bad_nodes\n",
    "\n",
    "        for buckets in [10, 15, 20]:\n",
    "            d = df_ip[(df_ip.p == p) & (df_ip.num_bad_netgroups == buckets)].dropna().head(1).to_dict(orient='records')\n",
    "            if d:\n",
    "                locals().update(d[0])\n",
    "                assert num_bad_netgroups == num_bad_asn, \"Internal error!\"\n",
    "                trends.append({'p': p, 'buckets': buckets, 'num_bad_nodes': num_bad_nodes, 'cost_increase': num_bad_nodes/num_bad_nodes_no_bucket-1})\n",
    "\n",
    "df_trends = pd.DataFrame(trends, index=None)\n",
    "g = sns.barplot(x='p', y='cost_increase', hue='buckets', hue_order=[20, 15, 10], palette=['tab:purple', 'tab:orange', 'tab:blue'], data=df_trends, width=0.7, dodge=True)\n",
    "g.set_xlabel('Targeted eclipse probability')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
